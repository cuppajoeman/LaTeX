\documentclass[11pt]{book}

\RequirePackage{silence}
\WarningFilter{remreset}{The remreset package}

\title{MAT223 - Linear Algebra}
\author{Callum Cassidy-Nolan}

\input{mynotes-preamble.tex}

\begin{document}

\input{mynotes-header.tex}

\chapter{Week 9}%
\label{chp:week_9}
% chapter week_9

Linear transformations and such

\section{Linear Transformations}%
\label{sec:linear_transformations}
% section linear_transformations

\begin{defn}[Linear Transformation]\index{Linear Transformation}\label{defn:linear_transformation}
    Let $V$ and $W$ be subspaces. A function $\mathcal{T} : V \to W $ is called
    a linear transformation if for all $\vec{u}, \vec{v} \in V$ and $a \in \mathbb{R}$ it satisfies
    \begin{enumerate}
        \item $\mathcal{T}(\vec{u} + \vec{v}) = \mathcal{T}(\vec{u}) + \mathcal{T}(\vec{v})$ 
        \item $\mathcal{T}(a \vec{u}) = a \mathcal{T}(\vec{u})$ 
    \end{enumerate}
\end{defn}

%TODO: Prove those two properties hold

\begin{thm}[Unique Matrix for LT's]\index{Unique Matrix for LT's}\label{thm:unique_matrix_for_lt_s}
    For any linear transformation $L : V \to W $ there is a matrix $A$  such that $L\left(\vec{x}\right) = A\vec{x}$ for all $\vec{x} \in V$ 
\end{thm}

\begin{eg}
    We'll show that $\mathcal{R}$ is a linear transformation where $\mathcal{R}$
    is a counter clockwise rotation of $\frac{\pi }{2}$ radians
    \begin{equation*}
       \mathcal{R}\left(\mat{ x \\ y }\right) = \mat{ 0 & -1 \\ 1 & 0 } \mat{ x \\ y }
    \end{equation*}
    Let $\vec{u}, \vec{v} \in \mathbb{R}^2$ we know that for some $x_1, y_1,
    x_2, y_2 \in \mathbb{R}$ that
    \begin{equation*}
        \vec{u} = \mat{ x_1 \\ y_1 } \text{ and } \vec{v} = \mat{ x_2 \\ y_2 }
    \end{equation*}
    \begin{align*}
        \mathcal{R}\left(\mat{ x_1 \\ y_1 }\right) + \mathcal{R}\left(\mat{ x_2
        \\ y_2 }\right) &= \mat{ -y_1 \\ x_1 } + \mat{ -y_2 \\ x_2 }\\
                        &= \mat{ - \left( y_1 + y_2 \right) \\ x_1 + x_2 }\\
    \end{align*}
    Which is exactly equal to $\mathcal{R}\left(\vec{u} + \vec{v}\right)$ as
    required, then let $\alpha \in \mathbb{R}$ and we know that
    \begin{equation*}
        \mathcal{R}\left(\alpha \vec{u}\right) = \mat{ -\alpha y_1 \\ \alpha x_1 }
    \end{equation*}
    But also that 
    \begin{equation*}
        \alpha \mathcal{R}\left(\vec{u}\right) = \mat{ -\alpha y_1 \\ \alpha x_1 }
    \end{equation*}
    So then we've shown that $\mathcal{R}\left(\alpha \vec{u}\right) = \alpha
    \mathcal{R }\left(\vec{u}\right)$ but also that $\mathcal{R}\left(\vec{u} + \vec{v}\right) = \mathcal{R}\left(\vec{u}\right) + \mathcal{R}\left(\vec{v}\right)$ as req'd
\end{eg}

\begin{eg}
    We'll show that $\mathcal{T} : \mathbb{R}^2 \to \mathbb{R}^2 $ where
    $\mathcal{T} \mat{ x \\ y } = \mat{ x + 2 \\ y }$ is not a linear
    transformation.

    Let $\vec{j} = \mat{ 0 \\ 0 }, \vec{k} = \mat{ 0 \\ 0 }$ we have that
    \begin{equation*}
        \mathcal{T}(\mat{ 0 \\ 0 } + \mat{ 0 \\ 0 }) = \mat{ 2 \\ 0 }       
    \end{equation*}
    But then we can see that 
    \begin{equation*}
        \mathcal{T}(\mat{ 0 \\ 0 }) + \mathcal{T}(\mat{ 0 \\ 0 }) = \mat{ 2 \\ 0
        } + \mat{ 2 \\ 0 } = \mat{ 4 \\ 0 }         
    \end{equation*}
    Then we conclude that $\mathcal{T}(\vec{j} + \vec{k}) \neq
    \mathcal{T}(\vec{j}) + \mathcal{T}(\vec{k})$ 
\end{eg}

\begin{eg}
    We'll show that $\mathcal{P}$ is a linear transformation 
    n
    \begin{note}
        We'll show that it is closed under addition and multiplication
    \end{note}
    \begin{equation*}
        \mathcal{P}(\mat{ x \\ y }) = \mathit{comp}_{\vec{u}} {\mat{ x \\ y }} 
    \end{equation*}
    Let $\vec{j}, \vec{k} \in \mathbb{R}^2$ we know that 
    \begin{equation*}
        \mathit{comp}_{\vec{u}} {\vec{j}}  = \left( \frac{\vec{u} \cdot
        \vec{j}}{\left\Vert \vec{u} \right\Vert^2} \right) \vec{u} \text{ and }
        \mathit{comp}_{\vec{u}} {\vec{k}}  = \left( \frac{\vec{u} \cdot
        \vec{k}}{\left\Vert \vec{u} \right\Vert^2} \right) \vec{u}
    \end{equation*}
    And thus their product yields
    \begin{align*}
        \mathit{comp}_{\vec{u}} {\vec{j}}  + \mathit{comp}_{\vec{u}} {\vec{k}}
        &= \left( \frac{\vec{u} \cdot \left( \vec{j} + \vec{k}
        \right)}{\left\Vert \vec{u} \right\Vert^2} \right) \vec{u}\\
    \end{align*}
    Which is equal to 
    \begin{equation*}
        \mathit{comp}_{\vec{u}} {\left( \vec{j} + \vec{k} \right)}     
    \end{equation*}
    We must then show that it holds under multiplication let $\alpha \in
    \mathbb{R}$ and we know that 
    \begin{equation*}
        \alpha \mathit{comp}_{\vec{u}} {\vec{j}} = \alpha \left( \frac{\vec{u}
        \cdot \vec{j}}{ \left\Vert \vec{u} \right\Vert^2} \right) \vec{u} =
        \left( \frac{\vec{u} \cdot \alpha \vec{j}}{\left\Vert \vec{u}
        \right\Vert^2} \right) \vec{u} = \mathit{comp}_{\vec{u}} {\alpha \vec{j}} 
    \end{equation*}
\end{eg}

\begin{eg}
    We'll show that $W : \mathbb{R}^2 \to \mathbb{R}^2 $ is not a linear transformation, where
    \begin{equation*}
        \mathcal{W}\left(\mat{ x \\ y }\right) = \mat{ x^2 \\ y }
    \end{equation*}
    Let $\vec{x} = \mat{ x \\ y }$, and $\alpha  \in \mathbb{R}$  we know that
    \begin{equation*}
        \mathcal{W}\left(\alpha \mat{ x \\ y }\right) = \alpha^2 \mat{ x^2 \\ y^2 } \neq \alpha \mat{ x^2 \\ y^2 } = \alpha \mathcal{W}\left(\mat{ x \\ y }\right) 
    \end{equation*}
\end{eg}

% section linear_transformations (end)

\section{Image}%
\label{sec:image}
% section image

\begin{defn}[Image]\index{Image}\label{defn:image}
    Let $L : V \to W $ be a transformation and let $X \subseteq V$ be a set. The
    \hlnotea{ image of the set $X$ under L }, denoted as $L\left(X\right)$, is
    the set
    \begin{equation*}
        L\left(X\right) = \left\{ \vec{x} \in W: \vec{x} =
        L\left(\vec{y}\right) \text{ for some  } \vec{y} \in X \right\}
    \end{equation*}
\end{defn}

Let $S = \left\{ \mat{ x \\ y }: 0 \le x, y \le 1 \right\}$ be a filled in unit
square in the first quadrant. And let $C = \left\{ \vec{0}, \vec{e_1},
\vec{e_2}, \vec{e_1} + \vec{e_2} \right\} \subseteq \mathbb{R}^2$ be the corners of the unit square 

\begin{ex}
    We'll find what $\mathcal{R}\left(C\right)$ is, by the definition of image we have that
    \begin{align*}
        \mathcal{R}\left(C\right) &= \left\{ \mathcal{R}\left(\vec{0}\right), \mathcal{R}\left(\vec{e_1}\right), \mathcal{R}\left(\vec{e_2}\right), \mathcal{R}\left(\vec{e_1} + \vec{e_2}\right) \right\}\\
                                  &= \left\{ \vec{0}, \vec{e_2}, -\vec{e_1}, \vec{e_2} - \vec{e_1} \right\}\\
    \end{align*}
\end{ex}

\begin{ex}
    We'll now find what $\mathcal{W}\left(C\right)$ (Notice that it doesn't have to be a linear transformation) is, again we will use the definition so we have
    \begin{align*}
        \mathcal{W}\left(C\right) = \left\{ \vec{0}, \vec{e_1}, \vec{e_2}, \vec{e_1} + \vec{e_2} \right\}
    \end{align*}
\end{ex}

\begin{ex}
    $\mathcal{T}\left(C\right) = \left\{ \mat{ 2 \\ 0 }, \mat{ 3 \\ 0 }, \mat{ 1 \\ 2 }, \mat{ 1 \\ 3 } \right\}$ (The square has been shifted right 2 units)
\end{ex}

\begin{ex}
    We'll now operate on $S$, to find $\mathcal{R}\left(S\right)$ we imagine all the vectors in $\mathbb{R}^2$ that have been rotated $\frac{\pi }{2}$ radians counter clockwise from the intial square, or we could also multiply by the rotation matrix, either way we get the set
    \begin{equation*}
        \mathcal{R}\left(S\right) = \left\{ \mat{ -y \\ x }: 0 \le x, y \le 1 \right\}
    \end{equation*}
\end{ex}

\begin{ex}
    For $\mathcal{T}\left(S\right)$ we can re-imagine how we determined $\mathcal{T}\left(C\right)$ but for all the points in the square, this gives us the full square shifted horizontally by two units, so we have
    \begin{equation*}
        \mathcal{T}\left(S\right) = \left\{ \mat{ x + 2 \\ y }: 0 \le x, y \le 1 \right\}
    \end{equation*}
\end{ex}

\begin{ex}
    As for $\mathcal{P}\left(S\right)$ this is a bit more complicated, so we'll break it into to parts, the first is algebreically and the other will be vizually.

    Algebraically we know $\mathit{proj}_{\vec{u}} {\mat{ x \\ y }} $ will look like
    \begin{equation*}
        \left( \frac{\vec{u} \cdot \mat{ x \\ y }}{\left\Vert \vec{u} \right\Vert^2} \right) \vec{u}
    \end{equation*}
    But $\vec{u} = \mat{ 2 \\ 3 }$ so then
    \begin{equation*}
        \mathit{proj}_{\vec{u}} {\mat{ x \\ y }}  = \left( \frac{2x + 3y}{13} \right) \mat{ 2 \\ 3 }
    \end{equation*}
    So then we can conclude that
    \begin{equation*}
        \mathcal{P}\left(S\right) = \left\{ \frac{2x + 3}{13} \mat{ 2 \\ 3 } : 0 \le x, y \le 1 \right\}    
    \end{equation*}
\end{ex}

\begin{ex}
    Let $\ell = \left\{ t \vec{a} + \left( 1 - t \right) \vec{b} \text{ for some } t \in \left[ 0, 1 \right] \right\}$ and $\mathcal{A}$ be a linear transformation, we know that $\mathcal{A}\left(\ell\right)$ represents all vectors that are in the range of the linear transformation, let $\vec{u} \in \ell$,  then we know that $\vec{u} = t \vec{a} + \left( 1 - t \right)\vec{b} \text{ for some } t \in \mathbb{R}$ then we know 
    \begin{align*}
        \mathcal{A}\left(\vec{u}\right) &= \mathcal{A}\left(t \vec{a} + \left( 1 - t \right) \vec{b}\right)\\
                                        &= t\mathcal{A}\left(\vec{a}\right) + \left( 1 - t \right) \mathcal{A}\left(\vec{b}\right)\\
    \end{align*}
    And since we know that $\mathcal{A}\left(\vec{a}\right), \mathcal{A}\left(\vec{b}\right)$ are just two transformed vectors, then this defines a new line segment with endpoints $\mathcal{A}\left(\vec{a}\right) \text{  and  } \mathcal{A}\left(\vec{b}\right)$. 
\end{ex}

\begin{ex}
    We'll now find the linear transformation that italicizes N, FIG below

    To determine the linear transformation we start with the fact that if $A$ is some matrix then $A \vec{e_i} $ results in the ith column of $A$.

    We then choose two points and see how they moved after the transormation, per the hint above we'll choose two vectors that reside on the x, y axis. ( Our origin is the corner of the N ), so our first vector will be $\mat{ 0 \\ 3 }$ and our second is $\mat{ 2 \\ 0 }$. Thus we know the following
    \begin{enumerate}
        \item $\mathcal{I}\left(\mat{ 0 \\ 3 }\right) = \mat{ 4 \\ 1 }$, but we know that applying a linear transformation is the same as just multiplying by some matrix so we know that
            \begin{equation*}
                \mat{ a & b \\ c & d } \mat{ 0 \\ 3 } = \mat{ 4 \\ 1 } \Leftrightarrow \mat{ 3b \\ 3d } = \mat{ 4 \\ 1 }
            \end{equation*}
            And so we conclude that $b = \frac{4}{3} \text{ and } d = \frac{1}{3}$ so we've determined the first column of the matrix
        \item $\mathcal{I}\left(\mat{ 2 \\ 0 }\right) = \mat{ 2 \\ 0 }$ thus we know that $2a = 2 \Leftrightarrow a = 1 \text{ and that } b = 0$ and we have our second column of the matrix.
    \end{enumerate}
    So now we now that the matrix must look like
    \begin{equation*}
        \mat{ 1 & \frac{4}{3} \\ 0 & \frac{1}{3} }                      
    \end{equation*}
\end{ex}

\subsection{From Transformation to Matrix}%
\label{sub:from_transformation_to_matrix}
% subsection from_transformation_to_matrix

We defined $\mathcal{P}$ as the $\mathit{proj}_{\mathit{span} {\vec{u}} }
{\vec{x}} $ where $\vec{u} = \mat{ 2 \\ 3 }$ and $\mathcal{R}$ be a rotation ccw
by $\frac{\pi }{2}$ radians. We'll now find the matricies which define each
transformation

\begin{eg}
    \begin{align*}
        \mathcal{P}\left(\vec{x}\right) &= \left( \frac{\vec{u} \cdot \mat{ x \\ y}}{\left\Vert \vec{u} \right\Vert^2} \right) \vec{u}\\
        &= \left( \frac{2x + 3y}{13} \right) \mat{ 2 \\ 3 }\\
        &=\frac{1}{13} \mat{ 4x + 6y \\ 6x + 9y }\\
    \end{align*}
    By observation we know the matrix must be
    \begin{equation*}
        \frac{1}{13} \mat{ 4 & 6 \\ 6 & 9 }
    \end{equation*}
\end{eg}

\begin{eg}
    We'll now find the matrix which defines the rotation $\mathcal{R}$, we start
    geometrically FIG below.

    We could also use the fact that any matrix $A$ times $\vec{e_i}$ is equal to
    the i-th column of the matrix $A$. And we know that rotating $\vec{e_1}$ ccw
    $\frac{\pi }{2}$ moves it to $\vec{e_2}$ and that $\vec{e_2}$ rotated
    becomes $-\vec{e_1}$ so then we can determine that the matrix must be 
    \begin{equation*}
        \mat{ 0 & -1 \\ 1 & 0 }
    \end{equation*}
\end{eg}

% subsection from_transformation_to_matrix (end)

\subsection{Composition of Transformations}%
\label{sub:composition_of_transformations}
% subsection composition_of_transformations

We know that $f \circ g\left(x\right) = f\left(g\left(x\right)\right)$ and so
we can determine that $\mathcal{P} \circ \mathcal{R} =
\mathcal{P}\left(\mathcal{R}\left(\vec{x}\right)\right) =
\mathcal{P}\left(\mathcal{R}\left(\vec{x}\right)\right) =
\mathcal{P}\left(\mathcal{R} \vec{x}\right) = \mathcal{P} \mathcal{R} \vec{x}$.
So we can determine 
\begin{equation*}
    \mathcal{P} \circ \mathcal{R} = \frac{1}{13}\mat{ 4 & 6 \\ 6 & 9 } \mat{ 0 & -1 \\ 1 & 0
    } = \frac{1}{13} \mat{ 6 & -4 \\ 9 & -6 }
\end{equation*}

And that 

\begin{equation*}
    \mathcal{R} \circ \mathcal{P} = \mat{ 0 & -1 \\ 1 & 1 } \frac{1}{13}
    \mat{ 4 & 6 \\ 6 & 9 } = \frac{1}{13} \mat{ -6 & -9 \\ 4 & 6 }
\end{equation*}

We notice that these matricies are certainly different and that $\mathcal{P}
\circ \mathcal{R}$ is first a projection onto $\vec{u}$ and then a rotation,
whereas $\mathcal{R} \circ \mathcal{P}$ is first a rotation, then a projection
onto $\vec{u}$.

% subsection composition_of_transformations (end)

% section image (end)

\begin{defn}[Range]\index{Range}\label{defn:range}
    The \hlnotea{range} of a linear transformatoin $T : V \to W $ is the set of
    vectors that $T$ can output. That is ,
    \begin{equation*}
        \mathit{range} {\left( T \right)} = \left\{ \vec{y} \in W : \vec{y} =
        T\left(\vec{x} \right) \text{ for some } x \in V  \right\}
    \end{equation*}
\end{defn}

\begin{defn}[Null Space]\index{Null Space}\label{defn:null_space}
    The \hlnotea{null space} or \hlnoteb{kernel} of a linear transformation $T :
    V\to W $ is the set of vectors that get mapped to zero under $T$. That is,
    \begin{equation*}
        \mathit{null} {\left( T \right)} = \left\{ \vec{x} \in V:
        T\left(\vec{x}\right) = \vec{0} \right\}
    \end{equation*}
\end{defn}

\begin{ex}
    Consider $P : \mathbb{R}^2 \to  \mathbb{R}^2$ where $\mathcal{P}$ is the
    projection onto the $\mathit{span} {\vec{u}} $ ( Remember $\vec{u} =
    \mat{ 2 \\ 3 }$ ), we'll determine the range and null space of $\mathcal{P}$.
    
    We know that the projection of any vector onto $\vec{u}$ will be equal to
    some scalar times $\vec{u}$ so we know that the range will be 
    \begin{equation*}
        \alpha \vec{u}, \text{ for all } a \in \mathbb{R}
    \end{equation*}

    As for the nullspace, we can think of what vectors will get mapped to zero
    under a projection onto $\vec{u}$ with a bit of thought, we determine that
    it must be all vectors who are orthogonal to $\vec{u}$ as their "shadow"
    will drop to the zero vector. We know that this will be all scalar
    multiples of a normal vector to $\vec{u}$ so we can say $\alpha \mat{ -3 \\
    2} \text{ for all } \alpha  \in \mathbb{R}$ or we could first take the image
    of $\mathcal{P}$ then rotate all of those vectors, so we could say that
    $\mathit{null} {\left( \mathcal{P} \right)} = $ Image of the $\mathit{range}
    {P} $ under $\mathcal{R}$ 
\end{ex}

\begin{eg}
    We let $T : R^{n} \to R^{m} $ be a linear transformation. We'll show that the
    null space of $T$ is a linear subspace and that the range of $T$ is as well, so
    we'll show it is closed under addition and multiplication.
\end{eg}

\begin{proof}
    Let $\vec{u}, \vec{x} \in \mathit{null} {\left( T \right)} $ 
    \begin{equation*}
        T\left(\vec{u}\right) = 0 \text{ and } T\left(\vec{x}\right) = 0
    \end{equation*}
    Taking the sum of the above equations we get $0 = T\left(\vec{u}\right) +
    T\left(\vec{x}\right) = \mathcal{T}\left(\vec{x} + \vec{y}\right)$ from the definition of linear transformation. But by the definition of null set, we can conclude that $\vec{x} + \vec{y} \in \mathit{null} {\left( T \right)} $ because $T\left(\vec{x} + \vec{y}\right) = 0$. 
    
    Let $\alpha \in \mathbb{R}$ and from above we know that $T\left(\vec{x}\right) = 0 \Leftrightarrow \alpha T\left(\vec{x}\right) = \alpha 0 = 0$ and since $T$ is a linear transformationwe can say that $T\left(\alpha \vec{x}\right) = 0$ so then we know that $\alpha \vec{x} \in \mathit{null} {\left( T \right)} $ 
\end{proof}

\begin{proof}
    Let $\vec{j}, \vec{k} \in \mathit{range} {\left( T \right)} $ so we know that 
    \begin{equation*}
        \vec{j} = T\left(\vec{x}\right) \text{ and } \vec{k} = T\left(\vec{x_1}\right) \text{ for some } \vec{x}, \vec{x_1} \in R^{n}
    \end{equation*}
    Then we know that $\vec{j} + \vec{k} = T\left(\vec{x} + \vec{x_1}\right)$ and thus we conclude that $\vec{j} + \vec{k} \in \mathit{range} {\left( T \right)} $ 

    Let $\alpha \in \mathbb{R}$ and we know that $\alpha \vec{j} = \alpha T\left(\vec{x}\right) = T\left(\alpha \vec{x}\right)$ and so $\alpha \vec{j} \in \mathit{range} {\left( T \right)} $ as required.
\end{proof}

\section{Tutorial 5}%
\label{sec:tutorial_5}
% section tutorial_5

\begin{enumerate}
    \item $\mathcal{B}$ is a basis for a subspace $V$ if $\mathcal{B}$ is linearly independent and $\mathit{span} {\left( \mathcal{B} \right)}  = V$. 
    \item 
        \begin{enumerate}
            \item We'll verify if the fectors in $\mathcal{B}$ are in a basis. So for it to be a basis it must be linearly independent, and we'll assume that we are looking for a basis for the subspace $\mathbb{R}^{3}$. We'll start by looking at the linear combinations of each vector in the matrix that gives the zero vector to determine independendence, so we have ( These are augmented matricies for the zero vector. )
                \begin{gather*}
                    \mat{ 1 & 0 & 2 \\ -1 & 1 & -2 \\ 0 & 0 & 1 } \Leftrightarrow \mat{ 1 & 0 & 2 \\ 0 & 1 & 0 \\ 0 & 0 & 1 }
                \end{gather*}
                And thus we have one solution to $\vec{0}$ and so these vectors are linearly independent, and they span $\mathbb{R}^{3}$. 

                We'll now focus our attention to $\mathcal{C}$ using the same process as above we get
                \begin{equation*}
                    \mat{ 1 & 0 & 2 \\ - & 1 & -2 \\ 1 & 1 & 3 } \Leftrightarrow \mat{ 1 & 0 & 2 \\ 0 & 1 & 0 \\ 0 & 1 & 1 } \Leftrightarrow \mat{ 1 & 0 & 2 \\ 0 & 1 & 0 \\ 0 & 0 & 1 }
                \end{equation*}
                And by the same reasoning as before $\mathcal{C}$  is a basis for $\mathbb{R}^{3}$. 
            \item We'll start by finding what $\left[ \vec{v} \right]_{\mathcal{E}}$ is, but we don't have to do much as $\vec{v} = 4\vec{e_1} - 4\vec{e_2} + 2\vec{e_3}$ so we know that $\left[ \vec{v} \right]_{\mathcal{E}} = \mat{ 4 \\ -4 \\ 2 }$.

                Moving to $\left[ \vec{v} \right]_{\mathcal{B}}$ we know we are looking for some $\alpha , \beta , \gamma \in \mathbb{R}$ that satisfy
                \begin{equation*}
                    \alpha \mat{ 1 \\ -1 \\ 0 } + \beta \mat{ 0 \\ 1 \\ 0 } + \gamma \mat{ 2 \\ -2 \\ 1 } = \mat{ 4 \\ -4 \\ 2 }
                \end{equation*}
                This relates to a system of equations, which is then stored in a matrix, so we have
                \begin{equation*}
                    \mat{ 1 & 0 & 2 & 4 \\ -1 & 1 & -2 & -4 \\ 0 & 0 & 1 & 2 } \Leftrightarrow \mat{ 1 & 0 & 2 & 4 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 2 }
                \end{equation*}
                And thus we conclude that $\gamma = 2 ,\beta = 0, \alpha = 4 = 2 \gamma  = 0$ and so $\left[ \vec{v} \right]_{\mathcal{B}} = \mat{ 0 \\ 0 \\ 2 }$. 

                Now we'll figure out $\left[ \vec{v} \right]_{\mathcal{C}}$ so again we apply the same idea to get the following matrix
                \begin{equation*}
                    \mat{ 1 & 0 & 2 & 4 \\ -1 & 1 & -2 & -4 \\ 1 & 1 & 3 & 2 } \Leftrightarrow \mat{ 1 & 0 & 2 & 4 \\ 0 & 1 & 0 & 0 \\ 0 & 1 & 1 & -2 } \Leftrightarrow \mat{ 1 & 0 & 2 & 4 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & -2 }
                \end{equation*}
                And now we conclude that $\gamma = -2, \beta = 0, \alpha = 8$ and thus we have
                \begin{equation*}
                    \left[ \vec{v} \right]_{\mathcal{C}} = \mat{ 8 \\ 0 \\ -2 }
                \end{equation*}
            \item To determine $\left[ 7 \vec{v} \right]_{\mathcal{E}}$ we know we are looking for the solution to 
                \begin{equation*}
                    \alpha_1 \vec{e_1} + \beta_1 \vec{e_2} + \gamma_1 \vec{e_3} = 7 \vec{v}
                \end{equation*}
                But previously we know that the coeficients should be when we were just looking for $\vec{v}$ multiplying that equation by 7 on both sides tells us that 
                \begin{equation*}
                    \left[ 7 \vec{v} \right]_{\mathcal{E}} = \mat{ 28 \\ -28 \\ 14 }
                \end{equation*}

                Then using the same process as above we can determine that 
                \begin{equation*}
                    \left[ 7\vec{v} \right]_{\mathcal{B}} = \mat{ 0 \\ 0 \\ 14 } \text{ and } \left[ \vec{v} \right]_{\mathcal{C}} = \mat{ 57  \\ 0 \\ -14 }
                \end{equation*}
            \item I would prefer to write my measurements of scalar multiples of $\vec{v}$ in terms of the $\mathcal{B}$ basis as I only have to do one calculation.
        \end{enumerate}
    \item Get help with this one
    \item I chose $\mathcal{E}_{3}$ as then I could represent the vector $\vec{v} = \mat{ 1 \\ .12 }$ as $\left[ \mat{ -11 \\ 12 } \right]_{\mathcal{E}_{3}}$ 
\end{enumerate}

% section tutorial_5 (end)

\subsection{Video Notes}%
\label{sub:video_notes}
% subsection video_notes



% subsection video_notes (end)

\subsection{Extra Textbook Questions}%
\label{sub:extra_textbook_questions}
% subsection extra_textbook_questions

\begin{ex}
    A1
    \begin{itemize}
        \item 
    \begin{gather*}
    \begin{gmatrix}[b]
        1 & 0 & -1 & 3 \\
        0 & 1 & 3 & 1 \\
        1 & 2 & 1 & 6 \\
        2 & 5 & 1 & 1 
        \rowops
        \add[-1]{0}{2}
        \add[-2]{0}{3}
    \end{gmatrix}
        \rightsquigarrow
    \begin{gmatrix}[b]
        1 & 0 & -1 & 3 \\
        0 & 1 & 3 & 1 \\
        0 & 2 & 2 & 3 \\
        0 & 5 & 3 & -5
        \rowops
        \add[-2]{1}{2}
        \add[-5]{1}{3}
    \end{gmatrix}
        \rightsquigarrow\\
    \begin{gmatrix}[b]
        1 & 0 & -1 & 3 \\
        0 & 1 & 3 & 1 \\
        0 & 0 & -4 & 1 \\
        0 & 0 & -12 & -10
        \rowops
        \add[-3]{2}{3}
    \end{gmatrix}
        \rightsquigarrow
    \begin{gmatrix}[b]
        1 & 0 & -1 & 3 \\
        0 & 1 & 3 & 1 \\
        0 & 0 & -4 & 1 \\
        0 & 0 & 0 & - 13
    \end{gmatrix}
    \end{gather*}
We can stop here because we see that in the bottom row we have $0 = -13$ which is certainly false so the system is inconsistent. So we can conclude that $\mat{ 3 \\ 1 \\ 6 \\ 1 }$ is not in the range of L.
        \item We'll figure out if $\mat{ 3 \\ -5 \\ 1 \\ 5 }$ is in the range of L, so we're looking for some vector in $\mathbb{R}^{4}$ that becomes that vector under L. Represented as an augmented matrix we get
        \begin{gather*}
            \begin{gmatrix}[b]
                1 & 0 & -1 & 3 \\
                0 & 1 & 3 & -5 \\
                1 & 2 & 1 & 1 \\
                2 & 5 & 1 & 5 
                \rowops
                \add[-2]{2}{3}
            \end{gmatrix}
            \rightsquigarrow
            \begin{gmatrix}[b]
            	1 & 0 & -1 & 3 \\
            	0 & 1 & 3 & -5 \\
            	-2 & -4 & -2 & -2 \\
            	0 & 1 & -1 & 3
                \rowops
                \add[2]{0}{2}
            \end{gmatrix}
            \rightsquigarrow \\
            \begin{gmatrix}[b]
            	1 & 0 & -1 & 3 \\
            	0 & 1 & 3 & -5 \\
            	0 & -4 & -4 & 4 \\
            	0 & 1 & -1 & 3 
                \rowops
                \add[4]{3}{2}
            \end{gmatrix}
            \rightsquigarrow 
            \begin{gmatrix}[b]
            	1 & 0 & -1 & 3 \\
            	0 & 1 & 3 & -5 \\
            	0 & 0 & -8 & 16 \\
            	0 & 1 & -1 & 3 
                \rowops
                \add[-1]{1}{3}
                \swap{2}{3}
            \end{gmatrix}
            \rightsquigarrow \\
            \begin{gmatrix}[b]
            	1 & 0 & -1 & 3 \\
            	0 & 1 & 3 & -5 \\
            	0 & 0 & -4 & 8 \\
            	0 & 0 & -8 & 16 
                \rowops
                \add[-2]{2}{3}
            \end{gmatrix}
            \rightsquigarrow 
            \begin{gmatrix}[b]
            	1 & 0 & -1 & 3 \\
            	0 & 1 & 3 & -5 \\
            	0 & 0 & -4 & 8 \\
            	0 & 0 & 0 & 0 
                \rowops
                \mult{2}{\cdot -\frac{1}{4}}
            \end{gmatrix}
            \rightsquigarrow 
            \begin{gmatrix}[b]
            	1 & 0 & -1 & 3 \\
            	0 & 1 & 3 & -5 \\
            	0 & 0 & 1 & -2 \\
            	0 & 0 & 0 & 0 
            \end{gmatrix}
        \end{gather*}
        And thus we know there is a solution so $\mat{ 3 \\ -5 \\ 1 \\ 5 }$ is in the range of $L$. 
        \item A2
            \begin{enumerate}
                \item We'll start with the range which we know looks like 
                \begin{gather*}
                    \left\{ \vec{x} \in \mathbb{R}^2: \vec{x} = \mat{ 2x_1 \\ -x_2 + 2x_3 } \right\}\\
                \end{gather*}
                We know that $2x_1$ and $-x_2 + 2x_3$ will span over all of $\mathbb{R}$ and so we know that the range looks like $\mathit{span} {\left\{ \mat{ 0 \\ 1 }, \mat{ 1 \\ 0 } \right\}} $ and a basis for the range is the set $\left\{ \mat{ 1 \\ 0 }, \mat{ 0 \\ 1 } \right\}$ \\
                As for the null space we know that x component will be zero only if $x1 = 0$ and that the y component will be zero if and only if $-x2 + 2x_3 = 0 \Leftrightarrow x_2 = 2x_3$ and we let $t \in \mathbb{R}$ and let $x_3 = t$ and our solutions to the zero vector look like
                \begin{gather*}
                    \left\{ \vec{x} \in \mathbb{R}^3: \vec{x} = \mat{ 0 \\ 2t \\ t } \right\}\\
                    \left\{ \vec{x} \in \mathbb{R}^3: \vec{x} = t\mat{ 0 \\ 2 \\ 1 } \right\}
                \end{gather*}
                Thus we conclude that a basis for the null space is  $\left\{ \mat{ 0 \\ 2 \\ 1 } \right\}$ 
            \end{enumerate}
    \end{itemize}
\end{ex}

% subsection extra_textbook_questions (end)

\subsection{Extra Handout}%
\label{sub:extra_handout}
% subsection extra_handout



% subsection extra_handout (end)

% chapter week_9 (end)

\chapter{Week 10}%
\label{chp:week_10}
% chapter week_10

\section{Lecture 1}%
\label{sec:lecture_1}
% section lecture_1

Null space \& Range

\begin{defn}[Induced Transformation]\index{Induced Transformation}\label{defn:induced_transformation}
    Let $M$ be an $n \times m $ matrix. We say $M$ \hlnotea{induces} a linear transfomration $T_{M} : \mathbb{R}^{m} \to \mathbb{R}^{M}$ defined by 
    \begin{equation*}
        \left[ T_{M}\vec{v} \right]_{\mathcal{E'}} = M\left[ \vec{v} \right]_{\mathcal{E}}  
    \end{equation*}
\end{defn}

\begin{eg}
    For example from the above definition if we have the vector $\vec{v} = \left[ \mat{ 1 \\ 2 } \right]_{\mathcal{B}}$ and we wanted to determine what $T_{M}\left(\vec{v}\right)$ is we must first translate $\left[ \mat{ 1 \\ 2 } \right]_{\mathcal{B}}$ to the standard basis. One method to do so is to find the matrix $X$ that translates from one basis to another. Then plug that new box of numbers into $M \left[ \vec{v} \right]_{\mathcal{E}}$ and we get our solution to $T_{M}\left(\vec{v}\right)$ 
\end{eg}

\begin{eg}
    The difference between $M\vec{v}$ and $M \left[ \vec{v} \right]_{\mathcal{E}}$ is the level of rigor? on the right we have something precise but on the left we don't. Also we can remember that $\left[ \vec{v} \right]_{\mathcal{E}}$ represents a box of numbers, the coordinates in the basis, where as $\vec{v}$ is assumed to be in the $\mathcal{E}$ basis but this could alos mean a vector just floating in space without any type of basis involved.
\end{eg}

\begin{eg}
    To determine what $\left[ T_{M}\vec{e_1} \right]_{\mathcal{E'}}$ use the fact that it's equal to $M\left[ \vec{e_1} \right]_{\mathcal{E}}$ and we know that this corresponds to the first column in $M$ as $\left[ \vec{e_1} \right]_{\mathcal{E}} = \mat{ 1 \\ 0 }$ 
\end{eg}

\begin{eg}
    We start by knowing that each column must be in the range of $T_{M}$ by inputting $\vec{e_i}$ into the equation. But we showed last time that the range is a subspace which means that if we find two elements in it, we know the sum and product of the vectors is also in the range, so we know that the span of each column is a susbset of the range. % TODO: why is this a susbset but not equal?
    We then want to see if the span of the columns is actually eqaual to the range, so we must show the other direction now. So let $\vec{x_{1}}, \vec{x_2} \in \mathit{range} {\left( T_{M} \right)} $ so we know that $\vec{x} = T_{M}\left(\vec{v}\right)$ for some $\vec{v} \in \mathbb{R}^{m} \Leftrightarrow \left[ \vec{x} \right]_{\mathcal{E'}} = \left[ T_{M}\left(\vec{v}\right) \right]_{\mathcal{E'}} \Leftrightarrow \left[ \vec{x} \right]_{\mathcal{E'}} = M \left[ \vec{v} \right]_{\mathcal{E}}$ and we recall that $M$ is only a list of vectors and that $\left[ \vec{v} \right]_{\mathcal{E}}$ is but a box of numbers and so using matrix vector multiplication we know that we just get a linear combination of the vectors as columns in $M$ and so we showed that $\vec{x}$ looks like a vector in the span of the columns of $M$ as we wanted.
\end{eg}

\begin{defn}[Fundamental Subspaces]\index{Fundamental Subspaces}\label{defn:fundamental_subspaces}
    Associated with any matrix $M$ are three fundamental subspaces: the \hlnotea{row space} of $M$,  denoted $row(M)$, is the span of the rows of $M$ the \hlnotea{column space} of M, denoted $col(M)$ , is the span of the columns of $M$; and the \hlnotea{null space} of $M$, denoted $\mathit{null} {\left( M \right)} $,  is the set of solutions to $M\vec{x} = 0$ 
\end{defn}

Let $A = \mat{ 1 & 0 & 0 \\ 0 & 1 & 0 }$ 

\begin{eg}
    We know that the row space is $\mathit{span} {\left\{ \mat{ 1 \\ 0 \\ 0 }, \mat{ 0 \\ 1 \\ 0 } \right\}} $ or
    
    $\left\{ \mat{ \alpha \\ \beta \\ 0 } \in \mathbb{R}^3, \text{ for some } \alpha, \beta \in \mathbb{R} \right\}$  
\end{eg}

\begin{eg}
    The column space of $A$ is $\mathit{span} {\left\{ \mat{ 1 \\ 0 }, \mat{ 0 \\ 1 }, \mat{ 0 \\ 0 } \right\}} $ or equivalently $\mathbb{R}^2$, we can tell instantly that the column space is not the same as the row space as one is in $\mathbb{R}^{3}$ where the othe is $\mathbb{R}^2$. 
\end{eg}

\begin{eg}
    We are looking for all the vectors that are perpendicular to $\mat{ 1 \\ 0 \\ 0 } \text{ and  } \mat{ 0 \\ 1 \\ 0 }$ so we require the the vectors $\vec{x} \in \mathbb{R}^3$ such that 
    \begin{gather*}
        \mat{ 1 \\ 0 \\ 0 } \cdot \mat{ x \\ y \\ z } = 0 \text{ and } \mat{ 0 \\ 1 \\ 0 } \cdot \mat{ x \\ y \\ z } = 0\\
        x + y = 0\\
        x = -y
    \end{gather*}
    And so we know all solutions are 
    \begin{equation*}
        \alpha \mat{ -1 \\ 1 \\ 0 } + \beta \mat{ 0 \\ 0 \\ 1 }, \text{ for all } \alpha, \beta \in \mathbb{R}
    \end{equation*}
\end{eg}

\begin{eg}
    For the null space of $A$ we know we are looking for all vectors $\mat{ x \\ y \\ z } \in \mathbb{R}^3$ so that
    \begin{gather*}
        A \mat{ x \\ y \\ z } = 0 \\
        \mat{ 1 & 0 & 0 \\ 0 & 1 & 0 } \mat{ x \\ y  \\ z } = 0\\
        \mat{ 1 \\ 0 }x  + \mat{ 0 \\ 1 }y   + \mat{ 0 \\ 0 }z   = \mat{ 0 \\ 0 }
    \end{gather*}
    So we determine that $x = y = 0$ but $z \in \mathbb{R}$ so we can think of this as the z axis coming out from the x y plane.
\end{eg}

\begin{eg}
    The range of $T_{A} : \mathbb{R}^3 \to \mathbb{R}^2 $ 
    \begin{gather*}
        \left\{ \vec{u} \in \mathbb{R}^2: T_{A}\left(\vec{x}\right) = \vec{u}, \text{ for some } \vec{x} \in \mathbb{R}^3 \right\} \\
        \left\{ \vec{u} \in \mathbb{R}^2: \mat{ 1 & 0 & 0 \\ 0 & 1 & 0 } = \vec{u}, \text{ for some } \vec{x} \in \mathbb{R}^3 \right\}
    \end{gather*}
    Which we know is saying that we are looking for the vectors $\vec{u}$  such that they are linear combinations of the columns of the matrix $A$ so equivalently we have that $\vec{u} \in \mathit{span} {\left\{ \vec{e_1}, \vec{e_2} \right\}} $ 

    We'll now look for the $\mathit{null} {\left( T_{A} \right)} $ we know that the nullspace is defined by 
    \begin{equation*}
        \left\{ \vec{x} \in \mathbb{R}^3 : T_{A}\left(\vec{x}\right) = 0 \right\}
    \end{equation*}
    So we're really looking for the vectors which satisfy $A \vec{x} = 0$, which is equivalent to $\mathit{null} {\left( A \right)} $ 
\end{eg}

Let $B = \mat{ 1 & 2 & 3 \\ 1 & 1 & 1 }$ $C = rref\left(B\right) = \mat{ 1 & 0 & -1 \\ 0 & 1 & 2 }$ 

\begin{eg}
    The row space of $B$ is equal to  $\mathit{span} {\left\{ \mat{ 1 \\ 2 \\ 3 }, \mat{ 1 \\ 1 \\ 1 } \right\}} $ and the row reducing involves the operations of addition and multiplication of the rows together, let $\vec{x}, \vec{u}$ represnt the first and second vectors in the span, we know that in the row reduced form (considering rows to be vectors) we have that the first row
    \begin{equation*}
        \mat{ 1 \\ 0 \\ -1 } = \vec{u} - \left( \vec{x} - \vec{u} \right)
    \end{equation*}
    and the second row
    \begin{equation*}
        \mat{ 0 \\ 1 \\ 2 } = \vec{x} - \vec{u}
    \end{equation*}
    Thus we know that the rows in the row reduced form are just linear combinations of the original rows, so we can say that
    \begin{equation*}
        row\left(B\right) = row\left(C\right)
    \end{equation*}
\end{eg}

\begin{eg}
    As for $\mathit{null} {\left( B \right)} = \left\{ \vec{x}: B \vec{x} = 0 \right\} $ the process of finding soltions to the zero vector will be identical to just row reducing, where it's an augmented matrix with 0's on the right side, so due to the same reasoning as before we know that they will have the same solutions as row operations don't change the number of solutions so we can say that 
    \begin{equation*}
        \mathit{null} {\left( B \right)}  = \mathit{null} {\left( C \right)} 
    \end{equation*}
\end{eg}

\begin{eg}
    Now if we are asked to compute the null space of $B$ then we can make things quite a bit easier on ourselves by using the row reduced matrix so well have
    \begin{gather*}
        \left\{ \vec{x} \in \mathbb{R}^3: B\vec{x} = 0 \right\}
    \end{gather*}
    So we're looking at the solutions to 
    \[
    \mat{ 1 & 0 & -1 \\ 0 & 1 & 2 } \mat{ x \\ y \\ z } = 0 
    \]
    Let $z = t$ for some $t \in \mathbb{R}$ then we determine $y = -2t$ and $x = t$ in general form we get
    \[
    t \mat{ 1 \\ -2 \\ 1 }, \text{ for all  } t \in \mathbb{R}
    \]
\end{eg}

Let $P = \mat{ 0 & 0 \\ 1 & 2 }$ and $Q = rref\left(P\right) = \mat{ 1 & 2 \\ 0 & 0 }$

\begin{eg}
    $col\left(P\right) = \mathit{span} {\left\{ \mat{ 0 \\ 1 }, \mat{ 0 \\ 2 } \right\}} = \mathit{span} {\left\{ \mat{ 0 \\ 1 } \right\}}$ and 
    $col\left(Q\right) = \mathit{span} {\left\{ \mat{ 1 \\ 0 } \right\}} $ We notice these are different so then row reduction can change the columns space unlike the row space.
\end{eg}

\begin{defn}[Rank]\index{Rank}\label{defn:rank}
    For a linear transformation $T : V \to W $,  the \hlnotea{rank} of $T$,  denoted $rank\left(T\right)$, is the dimension of the range of $T$ 

    For an $n \times m$ matrix $M$,  the \hlnotea{rank} of M, denoted $rank\left(M\right)$,  is the number of pivots in $rref\left(M\right)$ 
\end{defn}

Let $\mathcal{P}$ be the projection onto $\mathit{span} {\left\{ \vec{u} \right\}} $ where $\vec{u} = \mat{ 2 \\ 3 }$,  and let $\mathcal{R}$ be a rotation counter-clockwise by $\frac{\pi}{2}$ radians.

\begin{eg}
    $\mathit{range} {\left( \mathcal{P} \right)} $ is equal to $\mathit{span} {\vec{u}} $ because we know that every vector you choose will become some a scalar of $\mat{ 2 \\ 3 }$ thus the dimension is 1, so it's rank is 1.

    As for the rank of $\mathcal{R}$ we know that iwe we consider every vector in $\mathbb{R}^2$ and rotate them all $\frac{\pi}{2}$ radians then we still have all of $\mathbb{R}^2$ so we know that the rank is still 2.
\end{eg}

\begin{eg}
    We'll now find the rank of the matricies that go along with $\mathcal{P} \text{ and }\mathcal{R}$ first we know that the matricies are
    \begin{align*}
        \mathcal{P} = \frac{1}{13} \mat{ 4 & 6 \\ 6 & 9 } && \mathcal{R} = \mat{ 0 & -1 \\ 1 & 0 }
    \end{align*}
    Starting with $\mathcal{P}$ 
    \begin{gather*}
\begin{gmatrix}[b]
    4 & 6 \\
    6 & 9 
\end{gmatrix}
        \rightsquigarrow
\begin{gmatrix}[b]
    24 & 36 \\
    24 & 36 
\end{gmatrix}
        \rightsquigarrow
\begin{gmatrix}[b]
    24 & 36 \\
    0 & 0 
\end{gmatrix}
    \end{gather*}
    And so we know that there is one pivot so the rank of this matrix is 1

    As for $\mathcal{R}$ by swapping row one and row two we see that there must be two pivots and so the rank is 2.
\end{eg}


% section lecture_1 (end)

\section{Lecture 2}%
\label{sec:lecture_2}
% section lecture_2

Let $M$ be a matrix and recall that 
\begin{align*}
    row\left(M\right) = \mathit{span} {\text{ rows of M }}  && col\left(M\right) = \mathit{span} {\text{ cols of M }} 
\end{align*}

And that the following properties hold

\begin{align*}
    row\left(M\right) = row\left(rref\left(M\right)\right) && col\left(M\right) \neq col\left(rref\left(M\right)\right)
\end{align*}

Also we know that 
\begin{equation*}
    rank\left(M\right) = \text{ pivots of  } rref\left(M\right)
\end{equation*}

\begin{eg}
    If $M$ looked like this after row reduction we know it's rank is two (two pivots)
    \[
    \begin{gmatrix}[b]
    	a & b & c \\
    	d & e & f \\
    	j & i & k 
    \end{gmatrix}
    \rightsquigarrow
    \begin{gmatrix}[b]
    	1 & 0 & \alpha \\
    	0 & 1 & \beta \\
    	0 & 0 & 0 
    \end{gmatrix}
    \]
\end{eg}

\begin{crly}
    $\begin{WithArrows}
        rank\left(M\right) &= \text{ \# linearly independent columns of } rref\left(M\right)\\
        &= dim\left(col\left(rref\left(M\right)\right)\right) \Arrow{Solutions aren't lost}\\
        &= dim\left(col\left(M\right)\right) \\
        &= \text{ \# linearly independent columns of }M \\
        &= dim\left(row\left(rref\left(M\right)\right)\right) \Arrow{pivots are both \\horizontal and vertical} \\
        &= dim\left(row\left(M\right)\right) \\
    \end{WithArrows}$
\end{crly}

\begin{defn}[Transpose of a Matrix]\index{Transpose of a Matrix}\label{defn:transpose_of_a_matrix}
    The transpose of a matrix $M$ is the matrix $M^{T}$ whose rows are the columns of $M$ 
\end{defn}

\begin{eg}
    If 
    \[
    M = 
    \begin{bmatrix}
    	a & b \\
    	c & d 
    \end{bmatrix}
    \Leftrightarrow
    M^{T} = 
    \begin{bmatrix}
    	a & c \\
    	b & d 
    \end{bmatrix}
    \]
    Or 
    \[
    M = 
    \begin{bmatrix}
    	1 & 2 & 3 \\
    	4 & 5 & 6 
    \end{bmatrix}
    \Leftrightarrow
    M^{T} = 
    \begin{bmatrix}
    	1 & 4 \\
    	2 & 5 \\
    	3 & 6 
    \end{bmatrix}
    \]
\end{eg}

\begin{crly}
    \[
    rank\left(M\right) = rank\left(M^{T}\right)
    \]
    because $dim\left(col\left(M\right)\right) = dim\left(row\left(M\right)\right)$ and we know that $col\left(M\right) = row\left(M^{T}\right)$ 
\end{crly}

Find the rank of the following matricies
\begin{ex}
    $\mat{ 1 & 1 \\ 2 & 2 }$ we can see that there is one linearly independent vector so the dimension is 1 and rank is 1
\end{ex}

\begin{ex}
    $\mat{ 1 & 2 \\ 3 & 4 }$ we can see that there are 2 linearly independent vectors so dimension is 2 and rank is 2
\end{ex}

\begin{ex}
    $\mat{ 1 & 1 & 0 \\ 0 & 0 & 1 }$ We know that the dimension of the column space is 2 and thus rank is 2
\end{ex}

\begin{ex}
    $\mat{ 3 \\ 3 \\ 2 }$ the rank is 1, there is only one column
\end{ex}

Let $M$ be a $3 \times 4$ matrix whose rank is 3 does this mean that all of the columns of $M$ are linearly independent ?

No! if the matrix is $3 \times 4$ then we know that there are four columns, and if the rank is 3 we know 3 of the are linearly independent but we don't know about the last one, so we can't be sure.

What if $M$ was $4 \times 3$ then we could be sure that every column is linearly independent as we know that it must have 3 pivots and so it must be true.

\begin{thm}[Rank-nullity]\index{Rank-nullity}\label{thm:rank_nullity}
    The \hlnotea{nullity} of a matrix is the dimension of the null space.\\
    The rank-nullity theorem for a matrix $A$ states
    \[
    rank\left(A\right) + nullity\left(A\right) = \# \text{ of columns of  } A
    \]
\end{thm}

\begin{eg}
    If we have
    \begin{equation*}
        \begin{gmatrix}[b]
        	. & . & . & . & . \\
        	. & . & . & . & . \\
        	. & . & . & . & . 
        \end{gmatrix}
        \rightsquigarrow 
        \begin{gmatrix}[b]
        	1 & 0 & . & . & . \\
        	0 & 1 & . & . & . \\
        	0 & 0 & . & . & . 
        \end{gmatrix}
    \end{equation*}
    And let's assume that the last 3 columns are free variable columns so then we know that the solution to the zero vector will have three paramers $t_1, t_2, t_3 \in \mathbb{R}$ and so we know that the dimension of the nullspace will be 3 .
   % TODO : Question  Ask why this is true again. 
\end{eg}

\begin{eg}
    Let $\vec{u}, \vec{v} \in \mathbb{R}^{9}$ be linearly independent and $\vec{w} = 2\vec{u} - \vec{v}$ 
    \begin{equation*}
        A = 
        \begin{bmatrix}[c|c|c]
        	\vec{v} & \vec{u} & \vec{w} 
        \end{bmatrix}
    \end{equation*}
    We instantly know that the dimension of the column space of $A$ is 2 and so rank is 2 thus nullity is 1 by R-N-T

    As for $A^{T}$ we know that $A$ has two pivot columns and so $A^{T}$ also has 2, and so by R-N-T it has nullity of 7.
\end{eg}

\begin{thm}[Rank-nullity for linear transformations]\index{Rank-nullity for linear transformations}\label{thm:rank_nullity_for_linear_transformations}
    Let $T : V \to W $ be a linear transformation thus there is a matrix $A$ such that $T = T_{A}$ so 
    \begin{equation*}
        T\left(\vec{x}\right) = A\vec{x} \text{ for all } \vec{x}\in V
    \end{equation*}
    We know from \cref{thm:rank_nullity} that 
    \begin{align*}
        \text{ \# columns of  }A &= rank\left(A\right) + nullity\left(A\right) \\
                                 &= dim\left(col\left(A\right)\right) + dim\left(\mathit{null} {\left( A \right)} \right)\\
                                 &= dim\left( \mathit{range} {\left( T \right)}  \right) + dim\left( \mathit{null} {\left( T \right)}  \right) \\
    \end{align*}
    So we can finally say that 
    \begin{equation*}
        dim\left( \mathit{range} {\left( T \right)}  \right) + dim\left( \mathit{null} {\left( T \right)}  \right) = \text{ \# columns of }A
    \end{equation*}
\end{thm}

\subsection{Inverse Matrix}%
\label{sub:inverse_matrix}
% subsection inverse_matrix

\begin{defn}[Inverse Transformation]\index{Inverse Transformation}\label{defn:inverse_transformation}
    If it exsits, the inverse transformation of $T : R^{n} \to R^{m} $ is 
    \begin{equation*}
        T^{-1} : R^{m} \to R^{n} 
    \end{equation*}
    That satisfies the following properties 
    \begin{gather*}
        T\left(T^{-1}\left(\vec{x}\right)\right) = \vec{x} \text{ for all  } \vec{x} \in \mathbb{R}^{n}\\
        T^{-1}\left(T\left(\vec{y}\right)\right) = \vec{y} \text{ for all } \vec{y} \in \mathbb{R}^{m}
    \end{gather*}
    If $T = T_{A}$ and $T^{-1} = T_{B}$ then
    \begin{align*}
        \vec{x} &=  T\left(T^{-1}\left(\vec{x}\right)\right) \\
        &= T\left(B\vec{x}\right) \\
        &= AB\vec{x}
    \end{align*}
    But for this to be true for all $\vec{x} \in \mathbb{R}^{m}$ then we require
    \begin{equation*}
        AB = I
    \end{equation*}
    And if $A$  is $n \times m$ and $B$  is $m \times n$ then we know that $I$  is $n \times n$ \\
    Similarly $T^{-1}\left(T\left(\vec{y}\right)\right) = \vec{y} \Leftrightarrow BA = I$ \\
    Thus we say $B$ is the inverse Matirx of $A$ denoted $B = A^{}$ 
\end{defn}
\begin{defn}[Inverse Matrix]\index{Inverse Matrix}\label{defn:inverse_matrix}
    We say $B$  is the inverse matrix of $A$ denoted $B = A^{-1}$ if
    \begin{equation*}
        AB = BA = I
    \end{equation*}
\end{defn}

% subsection inverse_matrix (end)

% section lecture_2 (end)

\section{Tutorial 6}%
\label{sec:tutorial_6}
% section tutorial_6

\begin{enumerate}
    \item We say the function $T : \mathbb{R}^{n} \to \mathbb{R}^{m} $ is linear if we can say
    \begin{equation*}
        T\left(\vec{x} + \alpha\vec{y}\right) = T\left(\vec{x}\right) + \alpha T\left(\vec{y}\right) \text{ for all  } \vec{x}, \vec{y} \in \mathbb{R}n
    \end{equation*}
    \item
    \begin{enumerate}
        \item 
        \begin{proof}
            We'll show that $\mathcal{A}$ is linear so let $\vec{u}, \vec{v} \in \mathbb{R}^2$ and $\alpha \in \mathbb{R}$ we know that
            \begin{equation*}
                T\left(\vec{u} + \alpha \vec{v}\right) = T\left( \mat{ x_1 + \alpha x_2 \\ y1 + \alpha y_2 }\right) = \mat{ y_1 + \alpha y_2 \\ 0 \\ x_1 + \alpha x_2 } = T\left(\vec{u}\right) + \alpha T\left(\vec{v}\right)
            \end{equation*}
        \end{proof}
        \item We will show that $\mathcal{B}$ is not a linear transformation we know 
        \begin{equation*}
            \mathcal{B}\left(\mat{ -2 \\ 0 } + \mat{ 1 \\ 0 }\right) \neq \mathcal{B}\left(\mat{ -2 \\ 0 }\right) + \mathcal{B}\left(\mat{ 1 \\ 0 }\right)
        \end{equation*}
        \item We will show that $\mathcal{C}$ is a linear transformation it follows that 
        \begin{equation*}
            \mathcal{C}\left(\vec{u} + \alpha \vec{v}\right) = \vec{0} = \vec{0} + \alpha \vec{0} = \mathcal{C}\left(\vec{u}\right) + \alpha \mathcal{C}\left(\vec{v}\right)
        \end{equation*}
        \item We will show that $\mathcal{D}$ is not a linear transformation 
        \begin{equation*}
            \mathcal{D}\left(\vec{u} + \alpha \vec{v}\right) = \mat{ 1 \\ 1 } \neq \mat{ 1 \\ 1 } + \alpha \mat{ 1 \\ 1 } = \mathcal{D}\left(\vec{u}\right) + \alpha \mathcal{D}\left(\vec{u}\right)
        \end{equation*}
    \end{enumerate}
        \item We will now compute the null space, range and rank of each of the linear transformations
        \begin{enumerate}
            \item We'll start by comupting the range which we can we can say is
            \[
            \left\{ \vec{v} \in \mathbb{R}^3: \vec{v} = s \mat{ 1 \\ 0 \\ 0 } + t \mat{ 0 \\ 0 \\ 1 } \text{ for some } t, s \in \mathbb{R} \right\}
            \]
            we know that that null space is only $\vec{0}$, and that the rank must be 2 as a basis for the range is
            \[
            \left\{ \mat{ 1 \\ 0 \\ 0 }, \mat{ 0 \\ 0 \\ 1 } \right\}
            \]
            \item We'll start with the range
                % TODO : Question Is any of this right?
            \[
            \left\{ x \in \mathbb{R}: x \ge 0 \right\}
            \]
            And the null space is
            \[
            \left\{ x \in \mathbb{R}: x < 0 \right\}
            \]
            So then the rank of $\mathcal{B}$ is 0
            % TODO : Question Why?? 
            \item For $\mathcal{C}$ we know the range is
                \[
                \left\{ \vec{0} \right\}
                \]
                And then null space is 
                \[
                \left\{ \vec{x} \in \mathbb{R}^2 \right\}
                \]
                And then rank is 0
                % TODO : Question Why is it 0 shouldn't it be 1?
            \item The range of $\mathcal{D}$ is $\left\{ \mat{ 1 \\ 1 } \right\}$ and the null space is $\emptyset$ since $\mathcal{D}$ is not a subspace then there is no basis for it, so the rank doesn't exist.
        \end{enumerate}
        \item 
        \begin{enumerate}
            \item $\mathcal{X}$ : the tranformation invoked by the identity matrix.
            \item $\mathcal{Y}$ : cannot exist as we know $\mathit{range} {\left( Y \right)} \subseteq \mathbb{R}^2$ and so the dimension of $\mathcal{Y} \le 2$ 
            \item $\mathcal{Z}: \mathit{comp}_{\mat{ 1 \\ 1 \\ 1 }} {\vec{x}} $ which defines a line in 3d space thus has dimension 1 so rank is 1 
            \item $\mathcal{W}$: the identity transformation
            \item Impossible?
            % TODO : Question Ask bout this
        \end{enumerate}
\end{enumerate}


% section tutorial_6 (end)

% chapter week_10 (end)

\end{document}
