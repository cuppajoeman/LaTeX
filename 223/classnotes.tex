% !TEX TS-program = pdflatex
\documentclass[notoc,notitlepage]{tufte-book}
% \nonstopmode % uncomment to enable nonstopmode

\usepackage{classnotetitle}

\newcommand\mat[1]{\begin{bmatrix}#1\end{bmatrix}}

\title{MAT223 - Linear Algebra}
\author{Callum Cassidy-Nolan}
\subtitle{Classnotes for Summer 2019}
\credentials{Computer Science}
\institution{University of Toronto}

\input{latex-classnotes-preamble.tex}

\begin{document}
\input{latex-classnotes-header.tex}

\chapter{Week 9}%
\label{chp:week_9}
% chapter week_9

Linear transformations and such

\section{Linear Transformations}%
\label{sec:linear_transformations}
% section linear_transformations

\begin{defn}[Linear Transformation]\index{Linear Transformation}\label{defn:linear_transformation}
    Let $V$ and $W$ be subspaces. A function $\mathcal{T} : V \to W $ is called
    a linear transformation if for all $\vec{u}, \vec{v} \in V$ and $a \in \mathbb{R}$ it satisfies
    \begin{enumerate}
        \item $\mathcal{T}(\vec{u} + \vec{v}) = \mathcal{T}(\vec{u}) + \mathcal{T}(\vec{v})$ 
        \item $\mathcal{T}(a \vec{u}) = a \mathcal{T}(\vec{u})$ 
    \end{enumerate}
\end{defn}

\begin{eg}
    We'll show that $\mathcal{R}$ is a linear transformation where $\mathcal{R}$
    is a counter clockwise rotation of $\frac{\pi }{2}$ radians
    \begin{equation*}
       \mathcal{R}\left(\mat{ x \\ y }\right) = \mat{ 0 & -1 \\ 1 & 0 } \mat{ x \\ y }
    \end{equation*}
    Let $\vec{u}, \vec{v} \in \mathbb{R}^2$ we know that for some $x_1, y_1,
    x_2, y_2 \in \mathbb{R}$ that
    \begin{equation*}
        \vec{u} = \mat{ x_1 \\ y_1 } \text{ and } \vec{v} = \mat{ x_2 \\ y_2 }
    \end{equation*}
    \begin{align*}
        \mathcal{R}\left(\mat{ x_1 \\ y_1 }\right) + \mathcal{R}\left(\mat{ x_2
        \\ y_2 }\right) &= \mat{ -y_1 \\ x_1 } + \mat{ -y_2 \\ x_2 }\\
                        &= \mat{ - \left( y_1 + y_2 \right) \\ x_1 + x_2 }\\
    \end{align*}
    Which is exactly equal to $\mathcal{R}\left(\vec{u} + \vec{v}\right)$ as
    required, then let $\alpha \in \mathbb{R}$ and we know that
    \begin{equation*}
        \mathcal{R}\left(\alpha \vec{u}\right) = \mat{ -\alpha y_1 \\ \alpha x_1 }
    \end{equation*}
    But also that 
    \begin{equation*}
        \alpha \mathcal{R}\left(\vec{u}\right) = \mat{ -\alpha y_1 \\ \alpha x_1 }
    \end{equation*}
    So then we've shown that $\mathcal{R}\left(\alpha \vec{u}\right) = \alpha
    \mathcal{R }\left(\vec{u}\right)$ but also that $\mathcal{R}\left(\vec{u} + \vec{v}\right) = \mathcal{R}\left(\vec{u}\right) + \mathcal{R}\left(\vec{v}\right)$ as req'd
\end{eg}

\begin{eg}
    We'll show that $\mathcal{T} : \mathbb{R}^2 \to \mathbb{R}^2 $ where
    $\mathcal{T} \mat{ x \\ y } = \mat{ x + 2 \\ y }$ is not a linear
    transformation.

    Let $\vec{j} = \mat{ 0 \\ 0 }, \vec{k} = \mat{ 0 \\ 0 }$ we have that
    \begin{equation*}
        \mathcal{T}(\mat{ 0 \\ 0 } + \mat{ 0 \\ 0 }) = \mat{ 2 \\ 0 }       
    \end{equation*}
    But then we can see that 
    \begin{equation*}
        \mathcal{T}(\mat{ 0 \\ 0 }) + \mathcal{T}(\mat{ 0 \\ 0 }) = \mat{ 2 \\ 0
        } + \mat{ 2 \\ 0 } = \mat{ 4 \\ 0 }         
    \end{equation*}
    Then we conclude that $\mathcal{T}(\vec{j} + \vec{k}) \neq
    \mathcal{T}(\vec{j}) + \mathcal{T}(\vec{k})$ 
\end{eg}

\begin{eg}
    We'll show that $\mathcal{P}$ is a linear transformation 
    \sidenote{We'll show that it is closed under addition and multiplication}
    \begin{equation*}
        \mathcal{P}(\mat{ x \\ y }) = \mathit{comp}_{\vec{u}} {\mat{ x \\ y }} 
    \end{equation*}
    Let $\vec{j}, \vec{k} \in \mathbb{R}^2$ we know that 
    \begin{equation*}
        \mathit{comp}_{\vec{u}} {\vec{j}}  = \left( \frac{\vec{u} \cdot
        \vec{j}}{\left\Vert \vec{u} \right\Vert^2} \right) \vec{u} \text{ and }
        \mathit{comp}_{\vec{u}} {\vec{k}}  = \left( \frac{\vec{u} \cdot
        \vec{k}}{\left\Vert \vec{u} \right\Vert^2} \right) \vec{u}
    \end{equation*}
    And thus their product yields
    \begin{align*}
        \mathit{comp}_{\vec{u}} {\vec{j}}  + \mathit{comp}_{\vec{u}} {\vec{k}}
        &= \left( \frac{\vec{u} \cdot \left( \vec{j} + \vec{k}
        \right)}{\left\Vert \vec{u} \right\Vert^2} \right) \vec{u}\\
    \end{align*}
    Which is equal to 
    \begin{equation*}
        \mathit{comp}_{\vec{u}} {\left( \vec{j} + \vec{k} \right)}     
    \end{equation*}
    We must then show that it holds under multiplication let $\alpha \in
    \mathbb{R}$ and we know that 
    \begin{equation*}
        \alpha \mathit{comp}_{\vec{u}} {\vec{j}} = \alpha \left( \frac{\vec{u}
        \cdot \vec{j}}{ \left\Vert \vec{u} \right\Vert^2} \right) \vec{u} =
        \left( \frac{\vec{u} \cdot \alpha \vec{j}}{\left\Vert \vec{u}
        \right\Vert^2} \right) \vec{u} = \mathit{comp}_{\vec{u}} {\alpha \vec{j}} 
    \end{equation*}
\end{eg}

\begin{eg}
    We'll show that $W : \mathbb{R}^2 \to \mathbb{R}^2 $ is not a linear transformation, where
    \begin{equation*}
        \mathcal{W}\left(\mat{ x \\ y }\right) = \mat{ x^2 \\ y }
    \end{equation*}
    Let $\vec{x} = \mat{ x \\ y }$, and $\alpha  \in \mathbb{R}$  we know that
    \begin{equation*}
        \mathcal{W}\left(\alpha \mat{ x \\ y }\right) = \alpha^2 \mat{ x^2 \\ y^2 } \neq \alpha \mat{ x^2 \\ y^2 } = \alpha \mathcal{W}\left(\mat{ x \\ y }\right) 
    \end{equation*}
\end{eg}

% section linear_transformations (end)

\section{Image}%
\label{sec:image}
% section image

\begin{defn}[Image]\index{Image}\label{defn:image}
    Let $L : V \to W $ be a transformation and let $X \subseteq V$ be a set. The
    \hlnotea{ image of the set $X$ under L }, denoted as $L\left(X\right)$, is
    the set
    \begin{equation*}
        L\left(X\right) = \left\{ \vec{x} \in W: \vec{x} =
        L\left(\vec{y}\right) \text{ for some  } \vec{y} \in X \right\}
    \end{equation*}
\end{defn}

Let $S = \left\{ \mat{ x \\ y }: 0 \le x, y \le 1 \right\}$ be a filled in unit
square in the first quadrant. And let $C = \left\{ \vec{0}, \vec{e_1},
\vec{e_2}, \vec{e_1} + \vec{e_2} \right\} \subseteq \mathbb{R}^2$ be the corners of the unit square 

\begin{ex}
    We'll find what $\mathcal{R}\left(C\right)$ is, by the definition of image we have that
    \begin{align*}
        \mathcal{R}\left(C\right) &= \left\{ \mathcal{R}\left(\vec{0}\right), \mathcal{R}\left(\vec{e_1}\right), \mathcal{R}\left(\vec{e_2}\right), \mathcal{R}\left(\vec{e_1} + \vec{e_2}\right) \right\}\\
                                  &= \left\{ \vec{0}, \vec{e_2}, -\vec{e_1}, \vec{e_2} - \vec{e_1} \right\}\\
    \end{align*}
\end{ex}

\begin{ex}
    We'll now find what $\mathcal{W}\left(C\right)$ \sidenote{Notice that it doesn't have to be a linear transformation} is, again we will use the definition so we have
    \begin{align*}
        \mathcal{W}\left(C\right) = \left\{ \vec{0}, \vec{e_1}, \vec{e_2}, \vec{e_1} + \vec{e_2} \right\}
    \end{align*}
\end{ex}

\begin{ex}
    $\mathcal{T}\left(C\right) = \left\{ \mat{ 2 \\ 0 }, \mat{ 3 \\ 0 }, \mat{ 1 \\ 2 }, \mat{ 1 \\ 3 } \right\}$ \sidenote{The square has been shifted right 2 units}
\end{ex}

\begin{ex}
    We'll now operate on $S$, to find $\mathcal{R}\left(S\right)$ we imagine all the vectors in $\mathbb{R}^2$ that have been rotated $\frac{\pi }{2}$ radians counter clockwise from the intial square, or we could also multiply by the rotation matrix, either way we get the set
    \begin{equation*}
        \mathcal{R}\left(S\right) = \left\{ \mat{ -y \\ x }: 0 \le x, y \le 1 \right\}
    \end{equation*}
\end{ex}

\begin{ex}
    For $\mathcal{T}\left(S\right)$ we can re-imagine how we determined $\mathcal{T}\left(C\right)$ but for all the points in the square, this gives us the full square shifted horizontally by two units, so we have
    \begin{equation*}
        \mathcal{T}\left(S\right) = \left\{ \mat{ x + 2 \\ y }: 0 \le x, y \le 1 \right\}
    \end{equation*}
\end{ex}

\begin{ex}
    As for $\mathcal{P}\left(S\right)$ this is a bit more complicated, so we'll break it into to parts, the first is algebreically and the other will be vizually.

    Algebraically we know $\mathit{proj}_{\vec{u}} {\mat{ x \\ y }} $ will look like
    \begin{equation*}
        \left( \frac{\vec{u} \cdot \mat{ x \\ y }}{\left\Vert \vec{u} \right\Vert^2} \right) \vec{u}
    \end{equation*}
    But $\vec{u} = \mat{ 2 \\ 3 }$ so then
    \begin{equation*}
        \mathit{proj}_{\vec{u}} {\mat{ x \\ y }}  = \left( \frac{2x + 3y}{13} \right) \mat{ 2 \\ 3 }
    \end{equation*}
    So then we can conclude that
    \begin{equation*}
        \mathcal{P}\left(S\right) = \left\{ \frac{2x + 3}{13} \mat{ 2 \\ 3 } : 0 \le x, y \le 1 \right\}    
    \end{equation*}
\end{ex}

\begin{ex}
    Let $\ell = \left\{ t \vec{a} + \left( 1 - t \right) \vec{b} \text{ for some } t \in \left[ 0, 1 \right] \right\}$ and $\mathcal{A}$ be a linear transformation, we know that $\mathcal{A}\left(\ell\right)$ represents all vectors that are in the range of the linear transformation, let $\vec{u} \in \ell$,  then we know that $\vec{u} = t \vec{a} + \left( 1 - t \right)\vec{b} \text{ for some } t \in \mathbb{R}$ then we know 
    \begin{align*}
        \mathcal{A}\left(\vec{u}\right) &= \mathcal{A}\left(t \vec{a} + \left( 1 - t \right) \vec{b}\right)\\
                                        &= t\mathcal{A}\left(\vec{a}\right) + \left( 1 - t \right) \mathcal{A}\left(\vec{b}\right)\\
    \end{align*}
    And since we know that $\mathcal{A}\left(\vec{a}\right), \mathcal{A}\left(\vec{b}\right)$ are just two transformed vectors, then this defines a new line segment with endpoints $\mathcal{A}\left(\vec{a}\right) \text{  and  } \mathcal{A}\left(\vec{b}\right)$. 
\end{ex}

\begin{ex}
    We'll now find the linear transformation that italicizes N, FIG below

    To determine the linear transformation we start with the fact that if $A$ is some matrix then $A \vec{e_i} $ results in the ith column of $A$.

    We then choose two points and see how they moved after the transormation, per the hint above we'll choose two vectors that reside on the x, y axis. \sidenote{Our origin is the corner of the N}, so our first vector will be $\mat{ 0 \\ 3 }$ and our second is $\mat{ 2 \\ 0 }$. Thus we know the following
    \begin{enumerate}
        \item $\mathcal{I}\left(\mat{ 0 \\ 3 }\right) = \mat{ 4 \\ 1 }$, but we know that applying a linear transformation is the same as just multiplying by some matrix so we know that
            \begin{equation*}
                \mat{ a & b \\ c & d } \mat{ 0 \\ 3 } = \mat{ 4 \\ 1 } \Leftrightarrow \mat{ 3b \\ 3d } = \mat{ 4 \\ 1 }
            \end{equation*}
            And so we conclude that $b = \frac{4}{3} \text{ and } d = \frac{1}{3}$ so we've determined the first column of the matrix
        \item $\mathcal{I}\left(\mat{ 2 \\ 0 }\right) = \mat{ 2 \\ 0 }$ thus we know that $2a = 2 \Leftrightarrow a = 1 \text{ and that } b = 0$ and we have our second column of the matrix.
    \end{enumerate}
    So now we now that the matrix must look like
    \begin{equation*}
        \mat{ 1 & \frac{4}{3} \\ 0 & \frac{1}{3} }                      
    \end{equation*}
\end{ex}

\subsection{From Transformation to Matrix}%
\label{sub:from_transformation_to_matrix}
% subsection from_transformation_to_matrix

We defined $\mathcal{P}$ as the $\mathit{proj}_{\mathit{span} {\vec{u}} }
{\vec{x}} $ where $\vec{u} = \mat{ 2 \\ 3 }$ and $\mathcal{R}$ be a rotation ccw
by $\frac{\pi }{2}$ radians. We'll now find the matricies which define each
transformation

\begin{eg}
    \begin{align*}
        \mathcal{P}\left(\vec{x}\right) &= \left( \frac{\vec{u} \cdot \mat{ x \\ y}}{\left\Vert \vec{u} \right\Vert^2} \right) \vec{u}\\
        &= \left( \frac{2x + 3y}{13} \right) \mat{ 2 \\ 3 }\\
        &=\frac{1}{13} \mat{ 4x + 6y \\ 6x + 9y }\\
    \end{align*}
    By observation we know the matrix must be
    \begin{equation*}
        \frac{1}{13} \mat{ 4 & 6 \\ 6 & 9 }
    \end{equation*}
\end{eg}

\begin{eg}
    We'll now find the matrix which defines the rotation $\mathcal{R}$, we start
    geometrically FIG below.

    We could also use the fact that any matrix $A$ times $\vec{e_i}$ is equal to
    the i-th column of the matrix $A$. And we know that rotating $\vec{e_1}$ ccw
    $\frac{\pi }{2}$ moves it to $\vec{e_2}$ and that $\vec{e_2}$ rotated
    becomes $-\vec{e_1}$ so then we can determine that the matrix must be 
    \begin{equation*}
        \mat{ 0 & -1 \\ 1 & 0 }
    \end{equation*}
\end{eg}

% subsection from_transformation_to_matrix (end)

\subsection{Composition of Transformations}%
\label{sub:composition_of_transformations}
% subsection composition_of_transformations

We know that $f \circ g\left(x\right) = f\left(g\left(x\right)\right)$ and so
we can determine that $\mathcal{P} \circ \mathcal{R} =
\mathcal{P}\left(\mathcal{R}\left(\vec{x}\right)\right) =
\mathcal{P}\left(\mathcal{R}\left(\vec{x}\right)\right) =
\mathcal{P}\left(\mathcal{R} \vec{x}\right) = \mathcal{P} \mathcal{R} \vec{x}$.
So we can determine 
\begin{equation*}
    \mathcal{P} \circ \mathcal{R} = \frac{1}{13}\mat{ 4 & 6 \\ 6 & 9 } \mat{ 0 & -1 \\ 1 & 0
    } = \frac{1}{13} \mat{ 6 & -4 \\ 9 & -6 }
\end{equation*}

And that 

\begin{equation*}
    \mathcal{R} \circ \mathcal{P} = \mat{ 0 & -1 \\ 1 & 1 } \frac{1}{13}
    \mat{ 4 & 6 \\ 6 & 9 } = \frac{1}{13} \mat{ -6 & -9 \\ 4 & 6 }
\end{equation*}

We notice that these matricies are certainly different and that $\mathcal{P}
\circ \mathcal{R}$ is first a projection onto $\vec{u}$ and then a rotation,
whereas $\mathcal{R} \circ \mathcal{P}$ is first a rotation, then a projection
onto $\vec{u}$.

% subsection composition_of_transformations (end)

% section image (end)

\begin{defn}[Range]\index{Range}\label{defn:range}
    The \hlnotea{range} of a linear transformatoin $T : V \to W $ is the set of
    vectors that $T$ can output. That is ,
    \begin{equation*}
        \mathit{range} {\left( T \right)} = \left\{ \vec{y} \in W : \vec{y} =
        T\left(\vec{x} \right) \text{ for some } x \in V  \right\}
    \end{equation*}
\end{defn}

\begin{defn}[Null Space]\index{Null Space}\label{defn:null_space}
    The \hlnotea{null space} or \hlnoteb{kernel} of a linear transformation $T :
    V\to W $ is the set of vectors that get mapped to zero under $T$. That is,
    \begin{equation*}
        \mathit{null} {\left( T \right)} = \left\{ \vec{x} \in V:
        T\left(\vec{x}\right) = \vec{0} \right\}
    \end{equation*}
\end{defn}

\begin{ex}
    Consider $P : \mathbb{R}^2 \to  \mathbb{R}^2$ where $\mathcal{P}$ is the
    projection onto the $\mathit{span} {\vec{u}} $ \sidenote{Remember $\vec{u} =
    \mat{ 2 \\ 3 }$ }, we'll determine the range and null space of $\mathcal{P}$.
    
    We know that the projection of any vector onto $\vec{u}$ will be equal to
    some scalar times $\vec{u}$ so we know that the range will be 
    \begin{equation*}
        \alpha \vec{u}, \text{ for all } a \in \mathbb{R}
    \end{equation*}

    As for the nullspace, we can think of what vectors will get mapped to zero
    under a projection onto $\vec{u}$ with a bit of thought, we determine that
    it must be all vectors who are orthogonal to $\vec{u}$ as their "shadow"
    will drop to the zero vector. We know that this will be all scalar
    multiples of a normal vector to $\vec{u}$ so we can say $\alpha \mat{ -3 \\
    2} \text{ for all } \alpha  \in \mathbb{R}$ or we could first take the image
    of $\mathcal{P}$ then rotate all of those vectors, so we could say that
    $\mathit{null} {\left( \mathcal{P} \right)} = $ Image of the $\mathit{range}
    {P} $ under $\mathcal{R}$ 
\end{ex}

\begin{eg}
    We let $T : R^{n} \to R^{m} $ be a linear transformation. We'll show that the
    null space of $T$ is a linear subspace and that the range of $T$ is as well, so
    we'll show it is closed under addition and multiplication.
\end{eg}

\begin{proof}
    Let $\vec{u}, \vec{x} \in \mathit{null} {\left( T \right)} $ 
    \begin{equation*}
        T\left(\vec{u}\right) = 0 \text{ and } T\left(\vec{x}\right) = 0
    \end{equation*}
    Taking the sum of the above equations we get $0 = T\left(\vec{u}\right) +
    T\left(\vec{x}\right) = \mathcal{T}\left(\vec{x} + \vec{y}\right)$ from the definition of linear transformation. But by the definition of null set, we can conclude that $\vec{x} + \vec{y} \in \mathit{null} {\left( T \right)} $ because $T\left(\vec{x} + \vec{y}\right) = 0$. 
    
    Let $\alpha \in \mathbb{R}$ and from above we know that $T\left(\vec{x}\right) = 0 \Leftrightarrow \alpha T\left(\vec{x}\right) = \alpha 0 = 0$ and since $T$ is a linear transformationwe can say that $T\left(\alpha \vec{x}\right) = 0$ so then we know that $\alpha \vec{x} \in \mathit{null} {\left( T \right)} $ 
\end{proof}

\begin{proof}
    Let $\vec{j}, \vec{k} \in \mathit{range} {\left( T \right)} $ so we know that 
    \begin{equation*}
        \vec{j} = T\left(\vec{x}\right) \text{ and } \vec{k} = T\left(\vec{x_1}\right) \text{ for some } \vec{x}, \vec{x_1} \in R^{n}
    \end{equation*}
    Then we know that $\vec{j} + \vec{k} = T\left(\vec{x} + \vec{x_1}\right)$ and thus we conclude that $\vec{j} + \vec{k} \in \mathit{range} {\left( T \right)} $ 

    Let $\alpha \in \mathbb{R}$ and we know that $\alpha \vec{j} = \alpha T\left(\vec{x}\right) = T\left(\alpha \vec{x}\right)$ and so $\alpha \vec{j} \in \mathit{range} {\left( T \right)} $ as required.
\end{proof}

\section{Tutorial 5}%
\label{sec:tutorial_5}
% section tutorial_5

\begin{enumerate}
    \item $\mathcal{B}$ is a basis for a subspace $V$ if $\mathcal{B}$ is linearly independent and $\mathit{span} {\left( \mathcal{B} \right)}  = V$. 
    \item 
        \begin{enumerate}
            \item We'll verify if the fectors in $\mathcal{B}$ are in a basis. So for it to be a basis it must be linearly independent, and we'll assume that we are looking for a basis for the subspace $\mathbb{R}^{3}$. We'll start by looking at the linear combinations of each vector in the matrix that gives the zero vector to determine independendence, so we have \sidenote{These are augmented matricies for the zero vector.}
                \begin{gather*}
                    \mat{ 1 & 0 & 2 \\ -1 & 1 & -2 \\ 0 & 0 & 1 } \Leftrightarrow \mat{ 1 & 0 & 2 \\ 0 & 1 & 0 \\ 0 & 0 & 1 }
                \end{gather*}
                And thus we have one solution to $\vec{0}$ and so these vectors are linearly independent, and they span $\mathbb{R}^{3}$. 

                We'll now focus our attention to $\mathcal{C}$ using the same process as above we get
                \begin{equation*}
                    \mat{ 1 & 0 & 2 \\ - & 1 & -2 \\ 1 & 1 & 3 } \Leftrightarrow \mat{ 1 & 0 & 2 \\ 0 & 1 & 0 \\ 0 & 1 & 1 } \Leftrightarrow \mat{ 1 & 0 & 2 \\ 0 & 1 & 0 \\ 0 & 0 & 1 }
                \end{equation*}
                And by the same reasoning as before $\mathcal{C}$  is a basis for $\mathbb{R}^{3}$. 
            \item We'll start by finding what $\left[ \vec{v} \right]_{\mathcal{E}}$ is, but we don't have to do much as $\vec{v} = 4\vec{e_1} - 4\vec{e_2} + 2\vec{e_3}$ so we know that $\left[ \vec{v} \right]_{\mathcal{E}} = \mat{ 4 \\ -4 \\ 2 }$.

                Moving to $\left[ \vec{v} \right]_{\mathcal{B}}$ we know we are looking for some $\alpha , \beta , \gamma \in \mathbb{R}$ that satisfy
                \begin{equation*}
                    \alpha \mat{ 1 \\ -1 \\ 0 } + \beta \mat{ 0 \\ 1 \\ 0 } + \gamma \mat{ 2 \\ -2 \\ 1 } = \mat{ 4 \\ -4 \\ 2 }
                \end{equation*}
                This relates to a system of equations, which is then stored in a matrix, so we have
                \begin{equation*}
                    \mat{ 1 & 0 & 2 & 4 \\ -1 & 1 & -2 & -4 \\ 0 & 0 & 1 & 2 } \Leftrightarrow \mat{ 1 & 0 & 2 & 4 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 2 }
                \end{equation*}
                And thus we conclude that $\gamma = 2 ,\beta = 0, \alpha = 4 = 2 \gamma  = 0$ and so $\left[ \vec{v} \right]_{\mathcal{B}} = \mat{ 0 \\ 0 \\ 2 }$. 

                Now we'll figure out $\left[ \vec{v} \right]_{\mathcal{C}}$ so again we apply the same idea to get the following matrix
                \begin{equation*}
                    \mat{ 1 & 0 & 2 & 4 \\ -1 & 1 & -2 & -4 \\ 1 & 1 & 3 & 2 } \Leftrightarrow \mat{ 1 & 0 & 2 & 4 \\ 0 & 1 & 0 & 0 \\ 0 & 1 & 1 & -2 } \Leftrightarrow \mat{ 1 & 0 & 2 & 4 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & -2 }
                \end{equation*}
                And now we conclude that $\gamma = -2, \beta = 0, \alpha = 8$ and thus we have
                \begin{equation*}
                    \left[ \vec{v} \right]_{\mathcal{C}} = \mat{ 8 \\ 0 \\ -2 }
                \end{equation*}
            \item To determine $\left[ 7 \vec{v} \right]_{\mathcal{E}}$ we know we are looking for the solution to 
                \begin{equation*}
                    \alpha_1 \vec{e_1} + \beta_1 \vec{e_2} + \gamma_1 \vec{e_3} = 7 \vec{v}
                \end{equation*}
                But previously we know that the coeficients should be when we were just looking for $\vec{v}$ multiplying that equation by 7 on both sides tells us that 
                \begin{equation*}
                    \left[ 7 \vec{v} \right]_{\mathcal{E}} = \mat{ 28 \\ -28 \\ 14 }
                \end{equation*}

                Then using the same process as above we can determine that 
                \begin{equation*}
                    \left[ 7\vec{v} \right]_{\mathcal{B}} = \mat{ 0 \\ 0 \\ 14 } \text{ and } \left[ \vec{v} \right]_{\mathcal{C}} = \mat{ 57  \\ 0 \\ -14 }
                \end{equation*}
            \item I would prefer to write my measurements of scalar multiples of $\vec{v}$ in terms of the $\mathcal{B}$ basis as I only have to do one calculation.
        \end{enumerate}
    \item Get help with this one
    \item I chose $\mathcal{E}_{3}$ as then I could represent the vector $\vec{v} = \mat{ 1 \\ .12 }$ as $\left[ \mat{ -11 \\ 12 } \right]_{\mathcal{E}_{3}}$ 
\end{enumerate}

% section tutorial_5 (end)

% chapter week_9 (end)

\appendix

\backmatter

\fancyhead[LE]{\thepage \enspace \textsl{\leftmark}}

\nocite{*}

\bibliography{references}

\printindex

\end{document}
% vim:tw=80:fdm=syntax
